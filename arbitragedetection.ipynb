{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "731610f8-9783-4a8a-be3f-f5a6c39b9af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/21 10:55:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "\n",
    "# Initialize Spark session for Arbitrage Opportunities Detection with necessary configurations\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ArbitrageOpportunitiesDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-mllib_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f76d36c5-c44f-4007-bef8-b591ccb9a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/21 11:20:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/04/21 11:20:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/9j/0m5g2l6d03z4pd5y6pt7xh1h0000gn/T/temporary-e6091573-4d93-4dd2-89bb-9d2bf38adc4d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/21 11:20:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 217\n",
      "-------------------------------------------\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|  price|          symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|64800.1|KRAKEN_SPOT_BTC_USD|       BUY|2024-04-21 04:20:05|9743430a5b87fd1f7...|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITFINEX_SPOT_BTC...|2024-04-21 04:20:07|64749.0|8cae24eb23f8860df...|      SELL|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 218\n",
      "-------------------------------------------\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|  price|           symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|64744.0|BITFINEX_SPOT_BTC...|       BUY|2024-04-21 04:20:24|6ad02951ec3f1f12d...|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:20:07|64800.0|99e9343d9705dd653...|      SELL|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 219\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|  price|          symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|64800.1|KRAKEN_SPOT_BTC_USD|       BUY|2024-04-21 04:20:05|388489d2e8dd4baec...|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USDT|2024-04-21 04:20:42|64760.6|d270ef46fb0a2d732...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:20:05|64800.0|56db55dbc87630102...|      SELL|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITFINEX_SPOT_BTC...|2024-04-21 04:20:07|64749.0|537311e540a35033e...|      SELL|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITSTAMP_SPOT_BTC...|2024-04-21 04:20:13|64744.0|e54952334debb2bfd...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:20:05|64800.1|9743430a5b87fd1f7...|       BUY|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITSTAMP_SPOT_BTC...|2024-04-21 04:20:13|64744.0|f5507ba44b5a0f0fd...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITSTAMP_SPOT_BTC...|2024-04-21 04:20:13|64744.0|d7881a6b2876b74ab...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITFINEX_SPOT_BTC...|2024-04-21 04:20:24|64744.0|6ad02951ec3f1f12d...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITSTAMP_SPOT_BTC...|2024-04-21 04:20:24|64748.0|84dd0d83564e673b3...|      SELL|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:20:05|64800.1|758772de5c1cfcdd6...|       BUY|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITSTAMP_SPOT_BTC...|2024-04-21 04:20:13|64744.0|3aef499adfd8f53e2...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITFINEX_SPOT_BTC...|2024-04-21 04:20:43|64786.0|14db85b713aa0f9f5...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:20:05|64800.1|388489d2e8dd4baec...|       BUY|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 220\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|  price|          symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|64800.0|KRAKEN_SPOT_BTC_USD|      SELL|2024-04-21 04:20:07|b59d7bc9bd08e9f2e...|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:20:07|64800.0|b59d7bc9bd08e9f2e...|      SELL|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 221\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|  price|           symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|64783.0|BITSTAMP_SPOT_BTC...|      SELL|2024-04-21 04:21:14|26606c75c76cfc70b...|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITSTAMP_SPOT_BTC...|2024-04-21 04:21:14|64783.0|26606c75c76cfc70b...|      SELL|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITFINEX_SPOT_BTC...|2024-04-21 04:21:15|64782.0|09fea658b62909052...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 222\n",
      "-------------------------------------------\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|  price|           symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|64782.0|BITFINEX_SPOT_BTC...|       BUY|2024-04-21 04:21:15|09fea658b62909052...|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITSTAMP_SPOT_BTC...|2024-04-21 04:21:17|64794.0|5d6b2c6047558eac0...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 223\n",
      "-------------------------------------------\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|  price|           symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|64794.0|BITSTAMP_SPOT_BTC...|       BUY|2024-04-21 04:21:17|5d6b2c6047558eac0...|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 19\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 224\n",
      "-------------------------------------------\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|  price|          symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|64800.1|KRAKEN_SPOT_BTC_USD|       BUY|2024-04-21 04:21:20|76d107ab1781af992...|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:21:20|64800.1|76d107ab1781af992...|       BUY|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 20\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 225\n",
      "-------------------------------------------\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|  price|          symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|64800.1|KRAKEN_SPOT_BTC_USD|       BUY|2024-04-21 04:21:20|601c47b85672d88ba...|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:21:20|64800.1|601c47b85672d88ba...|       BUY|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 226\n",
      "-------------------------------------------\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|  price|           symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|64776.6|KRAKEN_SPOT_BTC_USDT|      SELL|2024-04-21 04:21:39|d638cf64e8bd68b87...|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 21\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USDT|2024-04-21 04:21:39|64776.6|d638cf64e8bd68b87...|      SELL|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 22\n",
      "-------------------------------------------\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:21:38|64800.1|c9c26f8a1ad04b412...|       BUY|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 227\n",
      "-------------------------------------------\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|  price|          symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|64800.1|KRAKEN_SPOT_BTC_USD|       BUY|2024-04-21 04:21:38|c9c26f8a1ad04b412...|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 23\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USDT|2024-04-21 04:21:39|64779.9|56b8467a9f3cdc0dc...|      SELL|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 228\n",
      "-------------------------------------------\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|  price|           symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|64779.9|KRAKEN_SPOT_BTC_USDT|      SELL|2024-04-21 04:21:39|56b8467a9f3cdc0dc...|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 24\n",
      "-------------------------------------------\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:21:45|64800.1|2a97e5ac457d55fc9...|       BUY|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 229\n",
      "-------------------------------------------\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|  price|          symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|64800.1|KRAKEN_SPOT_BTC_USD|       BUY|2024-04-21 04:21:45|2a97e5ac457d55fc9...|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 25\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|           symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "|BITSTAMP_SPOT_BTC...|2024-04-21 04:21:17|64794.0|36a4a990b82673fe6...|       BUY|\n",
      "+--------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 230\n",
      "-------------------------------------------\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|  price|           symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "|64794.0|BITSTAMP_SPOT_BTC...|       BUY|2024-04-21 04:21:17|36a4a990b82673fe6...|\n",
      "+-------+--------------------+----------+-------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 231\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 26\n",
      "-------------------------------------------\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|  price|          symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|64800.1|KRAKEN_SPOT_BTC_USD|       BUY|2024-04-21 04:21:54|5f0bce597e7dab5df...|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:21:54|64800.1|5f0bce597e7dab5df...|       BUY|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 27\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 232\n",
      "-------------------------------------------\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|          symbol_id|      time_exchange|  price|                uuid|taker_side|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "|KRAKEN_SPOT_BTC_USD|2024-04-21 04:22:17|64800.0|a1d2de33a75ccb3d6...|      SELL|\n",
      "+-------------------+-------------------+-------+--------------------+----------+\n",
      "\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|  price|          symbol_id|taker_side|      time_exchange|                uuid|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "|64800.0|KRAKEN_SPOT_BTC_USD|      SELL|2024-04-21 04:22:17|a1d2de33a75ccb3d6...|\n",
      "+-------+-------------------+----------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 48\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Display the processed stream\u001b[39;00m\n\u001b[1;32m     43\u001b[0m query \u001b[38;5;241m=\u001b[39m streaming_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 48\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session for Arbitrage Opportunities Detection with necessary configurations\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ArbitrageOpportunitiesDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-mllib_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema based on the output from the sample file\n",
    "schema = StructType([\n",
    "    StructField(\"symbol_id\", StringType(), True),\n",
    "    StructField(\"time_exchange\", LongType(), True),  # Epoch time in milliseconds\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"uuid\", StringType(), True),\n",
    "    StructField(\"taker_side\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Define the directory to monitor for new JSON files\n",
    "input_directory = \"/Users/priyasuresh/Documents/Documents_Priya’s MacBook Pro/SPRING2024/Data 228-Big Data/project/temp_folder/\"\n",
    "\n",
    "# Set up the DataFrame to read streaming data from the directory\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(schema) \\\n",
    "    .json(input_directory)\n",
    "\n",
    "# Convert time_exchange from epoch to timestamp if necessary\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "streaming_df = streaming_df.withColumn(\"time_exchange\", from_unixtime(col(\"time_exchange\") / 1000))\n",
    "\n",
    "# Display the processed stream\n",
    "query = streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c93eaf8-06e9-431b-979f-63e6c47dc222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: double (nullable = true)\n",
      " |-- symbol_id: string (nullable = true)\n",
      " |-- taker_side: string (nullable = true)\n",
      " |-- time_exchange: long (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing JSON files\n",
    "json_files_directory = \"/Users/priyasuresh/Documents/Documents_Priya’s MacBook Pro/SPRING2024/Data 228-Big Data/project/temp_folder/\"\n",
    "\n",
    "# Append the filename correctly with a slash\n",
    "sample_file_path = json_files_directory + \"batch_1_0.json\"\n",
    "\n",
    "# Read one file to infer schema\n",
    "sample_df = spark.read.json(sample_file_path)\n",
    "\n",
    "# Print the schema to use it in the streaming read\n",
    "sample_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc23d7fa-4cce-477c-9c93-a8628e54593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32566012-1faa-4a35-b36b-9e4f950d6554",
   "metadata": {},
   "source": [
    "### Final code to run for applying K-Means, LSH and final arbitrage calculation logic(extract every five minute lowest price for buy and highest price for sell among all three exchanges and calculate price difference and add as anew column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d569d9-ace6-43b4-8e42-c92a31607ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/24 23:42:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/24 23:42:35 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/9j/0m5g2l6d03z4pd5y6pt7xh1h0000gn/T/temporary-b3370f6d-fad4-425d-a85b-c2b2ae226e8f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/24 23:42:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "ERROR:root:Exception while sending command.                                     \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=78>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o15713.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o15751.awaitTermination",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 89\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Define the streaming query with a 5-minute processing time trigger\u001b[39;00m\n\u001b[1;32m     83\u001b[0m query \u001b[38;5;241m=\u001b[39m streaming_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(process_batch) \\\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 89\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o15751.awaitTermination"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime, unix_timestamp, min as spark_min, max as spark_max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ArbitrageOpportunitiesDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the incoming data\n",
    "schema = StructType([\n",
    "    StructField(\"symbol_id\", StringType(), True),\n",
    "    StructField(\"time_exchange\", LongType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"uuid\", StringType(), True),\n",
    "    StructField(\"taker_side\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Directory containing the JSON files\n",
    "input_directory = \"/Users/priyasuresh/Documents/Documents_Priya’s MacBook Pro/SPRING2024/Data 228-Big Data/project/temp_folder\"\n",
    "\n",
    "# Read streaming data\n",
    "streaming_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .json(input_directory) \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"time_exchange\") / 1000)) \\\n",
    "    .withColumn(\"time_minute\", (unix_timestamp(\"timestamp\") / 60).cast(\"integer\"))\n",
    "\n",
    "# Assemble features for K-Means and LSH\n",
    "vecAssembler = VectorAssembler(inputCols=[\"price\"], outputCol=\"price_features\")\n",
    "scaler = StandardScaler(inputCol=\"price_features\", outputCol=\"scaled_price_features\")\n",
    "kmeans = KMeans(featuresCol=\"scaled_price_features\", predictionCol=\"time_cluster\", k=5, seed=42)\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"scaled_price_features\", outputCol=\"hashes\", bucketLength=0.1, numHashTables=3)\n",
    "\n",
    "# Combine all stages into a pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler, scaler, kmeans, brp])\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    # Fit the pipeline model and transform the DataFrame\n",
    "    if df.count() > 0:\n",
    "        model = pipeline.fit(df)\n",
    "        transformed_df = model.transform(df)\n",
    "\n",
    "        # Aggregate to find minimum buy price and maximum sell price within each cluster\n",
    "        min_prices = transformed_df.filter(col(\"taker_side\") == \"BUY\").groupBy(\"time_cluster\").agg(spark_min(\"price\").alias(\"min_buy_price\"))\n",
    "        max_prices = transformed_df.filter(col(\"taker_side\") == \"SELL\").groupBy(\"time_cluster\").agg(spark_max(\"price\").alias(\"max_sell_price\"))\n",
    "\n",
    "        # Join min and max prices with original data\n",
    "        result_df = transformed_df.join(min_prices, \"time_cluster\", \"left_outer\").join(max_prices, \"time_cluster\", \"left_outer\")\n",
    "\n",
    "        # Calculate price difference\n",
    "        result_df = result_df.withColumn(\"price_diff\", col(\"max_sell_price\") - col(\"min_buy_price\"))\n",
    "\n",
    "        # Fill in missing values with defaults\n",
    "        default_values = {\n",
    "            \"min_buy_price\": 0.0,  # Default value for min_buy_price if missing\n",
    "            \"max_sell_price\": 0.0,  # Default value for max_sell_price if missing\n",
    "            \"price_diff\": 0.0,  # Default value for price_diff if missing\n",
    "            # Include defaults for any other fields as necessary\n",
    "        }\n",
    "        result_df = result_df.na.fill(default_values)\n",
    "\n",
    "        # Select necessary columns and write to JSON\n",
    "        result_df.select(\n",
    "            \"symbol_id\", \"time_exchange\", \"timestamp\", \"uuid\", \"taker_side\", \"price\",\n",
    "            \"min_buy_price\", \"max_sell_price\", \"price_diff\", \"time_cluster\", \"hashes\"\n",
    "        ).write.mode('append').json(\"/Users/priyasuresh/Documents/Documents_Priya’s MacBook Pro/SPRING2024/Data 228-Big Data/project/last_folder/clusters.json\")\n",
    "    else:\n",
    "        print(f\"No data found in batch {epoch_id}. Will check again in 5 minutes.\")\n",
    "\n",
    "\n",
    "# Define the streaming query with a 5-minute processing time trigger\n",
    "query = streaming_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .trigger(processingTime=\"5 minutes\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba99628b-8195-4c10-ab14-6f91d633b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 00:10:00 ERROR MicroBatchExecution: Query [id = 0207d0fe-ac79-42cd-a0ae-3eef4f566758, runId = 02138f72-3e66-490b-9163-67226e4216dd] terminated with error\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.buildReader(JsonFileFormat.scala:99)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:30)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/04/25 00:10:00 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@5409ba54[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@6c84004c[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@16976f1]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@6668080[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "Exception in thread \"stream execution thread for [id = 0207d0fe-ac79-42cd-a0ae-3eef4f566758, runId = 02138f72-3e66-490b-9163-67226e4216dd]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:426)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:360)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:340)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)\n",
      "\t... 11 more\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c6cff-1085-4c99-bb49-391fe9c6f466",
   "metadata": {},
   "source": [
    "### Final Code--logic changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72f1c37-4121-4fb0-aa9b-55ddac169010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/27 12:13:58 WARN Utils: Your hostname, Priyas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.68.79 instead (on interface en0)\n",
      "24/04/27 12:13:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/27 12:13:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/27 12:13:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/27 12:14:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/9j/0m5g2l6d03z4pd5y6pt7xh1h0000gn/T/temporary-520d9a0d-b1a3-441a-910f-9526561945c7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/27 12:14:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/27 12:14:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "ERROR:root:Exception while sending command.                                     \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=78>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o18.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/priyasuresh/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o58.awaitTermination",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 89\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Define the streaming query with a 5-minute processing time trigger\u001b[39;00m\n\u001b[1;32m     83\u001b[0m query \u001b[38;5;241m=\u001b[39m streaming_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(process_batch) \\\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 89\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o58.awaitTermination"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime, unix_timestamp, min as spark_min, max as spark_max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ArbitrageOpportunitiesDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the incoming data\n",
    "schema = StructType([\n",
    "    StructField(\"symbol_id\", StringType(), True),\n",
    "    StructField(\"time_exchange\", LongType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"uuid\", StringType(), True),\n",
    "    StructField(\"taker_side\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Directory containing the JSON files\n",
    "input_directory = \"/Users/priyasuresh/Documents/Documents_Priya’s MacBook Pro/SPRING2024/Data 228-Big Data/project/temp_folder\"\n",
    "\n",
    "# Read streaming data\n",
    "streaming_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .json(input_directory) \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"time_exchange\") / 1000)) \\\n",
    "    .withColumn(\"time_minute\", (unix_timestamp(\"timestamp\") / 60).cast(\"integer\"))\n",
    "\n",
    "# Assemble features for K-Means and LSH\n",
    "vecAssembler = VectorAssembler(inputCols=[\"price\"], outputCol=\"price_features\")\n",
    "scaler = StandardScaler(inputCol=\"price_features\", outputCol=\"scaled_price_features\")\n",
    "kmeans = KMeans(featuresCol=\"scaled_price_features\", predictionCol=\"time_cluster\", k=5, seed=42)\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"scaled_price_features\", outputCol=\"hashes\", bucketLength=0.1, numHashTables=3)\n",
    "\n",
    "# Combine all stages into a pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler, scaler, kmeans, brp])\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    # Fit the pipeline model and transform the DataFrame\n",
    "    if df.count() > 0:\n",
    "        model = pipeline.fit(df)\n",
    "        transformed_df = model.transform(df)\n",
    "\n",
    "        # Aggregate to find minimum buy price and maximum sell price within each cluster for each exchange\n",
    "        min_prices = transformed_df.filter(col(\"taker_side\") == \"BUY\").groupBy(\"symbol_id\", \"time_cluster\").agg(spark_min(\"price\").alias(\"min_buy_price\"))\n",
    "        max_prices = transformed_df.filter(col(\"taker_side\") == \"SELL\").groupBy(\"symbol_id\", \"time_cluster\").agg(spark_max(\"price\").alias(\"max_sell_price\"))\n",
    "\n",
    "        # Join min and max prices with original data\n",
    "        result_df = transformed_df.join(min_prices, [\"symbol_id\", \"time_cluster\"], \"left_outer\").join(max_prices, [\"symbol_id\", \"time_cluster\"], \"left_outer\")\n",
    "\n",
    "        # Calculate price difference\n",
    "        result_df = result_df.withColumn(\"price_diff\", col(\"max_sell_price\") - col(\"min_buy_price\"))\n",
    "\n",
    "        # Fill in missing values with defaults\n",
    "        default_values = {\n",
    "            \"min_buy_price\": 0.0,  # Default value for min_buy_price if missing\n",
    "            \"max_sell_price\": 0.0,  # Default value for max_sell_price if missing\n",
    "            \"price_diff\": 0.0,  # Default value for price_diff if missing\n",
    "        }\n",
    "        result_df = result_df.na.fill(default_values)\n",
    "\n",
    "        # Select necessary columns and write to JSON\n",
    "        result_df.select(\n",
    "            \"symbol_id\", \"time_exchange\", \"timestamp\", \"uuid\", \"taker_side\", \"price\",\n",
    "            \"min_buy_price\", \"max_sell_price\", \"price_diff\", \"time_cluster\", \"hashes\"\n",
    "        ).write.mode('append').json(\"/Users/priyasuresh/Documents/Documents_Priya’s MacBook Pro/SPRING2024/Data 228-Big Data/project/last_folder/clusters.json\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"No data found in batch {epoch_id}. Will check again in 5 minutes.\")\n",
    "\n",
    "\n",
    "# Define the streaming query with a 5-minute processing time trigger\n",
    "query = streaming_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .trigger(processingTime=\"5 minutes\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04eff3bc-3d80-4200-8d08-16462d238778",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b323ce9-96ee-495c-bc36-b996345430fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (pyenv)",
   "language": "python",
   "name": "pyenv3104"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
